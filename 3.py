from datetime import datetime, timedelta
from urllib.parse import urlencode
import requests
import xarray as xr
import numpy as np
import pandas as pd

# –ß—Ç–æ–±—ã z —Å–∫–∞—á–∞—Ç—å –æ–¥–Ω—É –¥–∞—Ç—É –∏ –æ–¥–Ω–æ –≤—Ä–µ–º—è —Å –ù–û–ê–ê

# –ü–æ–ª—É—á–∞–µ–º –¥–≤–∞ –≤—Ä–µ–º–µ–Ω–∏
times = datetime(2025, 5, 30, 18, 0, 0)
#print(times)
#times = times.strftime('%Y-%m-%d %H:%M')

def prepare_graphcast_coordinates(ds, base_datetime_str):
    # ‚úÖ –î–æ–±–∞–≤–ª—è–µ–º –æ—Å—å batch, –µ—Å–ª–∏ –µ—ë –µ—â—ë –Ω–µ—Ç
    if "batch" not in ds.dims:
        ds = ds.expand_dims(dim={"batch": [0]})

    # üéØ –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º time –≤ timedelta64[h]
    if not np.issubdtype(ds["time"].dtype, np.timedelta64):
        ds = ds.assign_coords(time=ds["time"].astype("timedelta64[h]"))

    # üéØ datetime = base_time + time
    base_time = np.datetime64(base_datetime_str)
    datetime = ds["time"].values + base_time

    ds["datetime"] = (("batch", "time"), np.expand_dims(datetime, axis=0))
    ds = ds.set_coords("datetime")

    return ds

# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã
# = datetime.date(2025, 4, 29)
date = times.date()
run_hour = times.strftime('%H')  # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —Å—Ç—Ä–æ–∫—É –≤–∏–¥–∞ "06"   –ó–∞–ø—É—Å–∫ –ø—Ä–æ–≥–Ω–æ–∑–∞: 00, 06, 12, 18
forecast_hour = "f000"  # –ê–Ω–∞–ª–∏–∑ (–±–µ–∑ –ø—Ä–æ–≥–Ω–æ–∑–∞)

# –°–ø–∏—Å–æ–∫ —É—Ä–æ–≤–Ω–µ–π
levels = [
    'lev_1000_mb', 'lev_925_mb', 'lev_850_mb', 'lev_700_mb', 'lev_600_mb',
    'lev_500_mb', 'lev_400_mb', 'lev_300_mb', 'lev_250_mb', 'lev_200_mb',
    'lev_150_mb', 'lev_100_mb', 'lev_50_mb',
    'lev_surface', 'lev_2_m_above_ground', 'lev_10_m_above_ground',
    'lev_mean_sea_level'
]

# –°–ø–∏—Å–æ–∫ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö (–≤ —Ñ–æ—Ä–º–∞—Ç–µ, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–Ω–∏–º–∞–µ—Ç NOMADS)
variables = [
    'var_TMP', 'var_HGT', 'var_UGRD', 'var_VGRD',
    'var_VVEL', 'var_SPFH', 'var_PRATE',
    'var_DSWRF', 'var_LAND', 'var_PRMSL'
]

# –†–µ–≥–∏–æ–Ω
region = {
    'subregion': '',
    'leftlon': 0,
    'rightlon': 359,
    'toplat': 90,
    'bottomlat': -90,
}

# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∑–∞–ø—Ä–æ—Å–∞
params = {
    'file': f"gfs.t{run_hour}z.pgrb2.1p00.{forecast_hour}",
    **{lvl: "on" for lvl in levels},
    **{var: "on" for var in variables},
    **region,
    'dir': f"/gfs.{date.strftime('%Y%m%d')}/{run_hour}/atmos"
}
print('–ø–∞—Ä–∞–º–µ—Ç—Ä—ã = ', params)
# –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å—Å—ã–ª–∫–∏
base_url = "https://nomads.ncep.noaa.gov/cgi-bin/filter_gfs_1p00.pl?"
download_url = base_url + urlencode(params)

print(download_url)
#--------------------------------------------------------------------------------------------------

# üîó –°—Å—ã–ª–∫–∞, —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Ä–∞–Ω–µ–µ (—Å—é–¥–∞ –ø–æ–¥—Å—Ç–∞–≤—å —Å–≤–æ—é)
#url = "https://nomads.ncep.noaa.gov/cgi-bin/filter_gfs_1p00.pl?file=gfs.t00z.pgrb2.1p00.f000&lev_1000_mb=on&var_TMP=on&subregion=&leftlon=0&rightlon=359&toplat=90&bottomlat=-90&dir=%2Fgfs.20250420%2F00%2Fatmos"
url = download_url

# üìÅ –ù–∞–∑–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–∞ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
output_filename = "c:/Users/obedenok/PycharmProjects/Graphcast/temp/gfs_1p00_f000_lll.grib2"

# üì• –°–∫–∞—á–∏–≤–∞–µ–º
print("üì• –°–∫–∞—á–∏–≤–∞–Ω–∏–µ –Ω–∞—á–∞–ª–æ—Å—å...")
response = requests.get(url)

# üßæ –°–æ—Ö—Ä–∞–Ω—è–µ–º
with open(output_filename, "wb") as f:
    f.write(response.content)

print(f"‚úÖ –§–∞–π–ª —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {output_filename}")
#---------------------------------------------------------------------------------------------------

grib_path = "c:/Users/obedenok/PycharmProjects/Graphcast/temp/gfs_1p00_f000_lll.grib2"

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–∏ —Å —Ñ–∏–ª—å—Ç—Ä–æ–º
def load_dataset(typeOfLevel, var_suffix):
    try:
        ds = xr.open_dataset(
            grib_path,
            engine="cfgrib",
            decode_timedelta=False,
            backend_kwargs = {"indexpath": ""},
            filter_by_keys={'typeOfLevel': typeOfLevel}
        )
        # –ü–µ—Ä–µ–∏–º–µ–Ω—É–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ —Å —Å—É—Ñ—Ñ–∏–∫—Å–æ–º, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –∫–æ–Ω—Ñ–ª–∏–∫—Ç–æ–≤
        return ds.rename({k: f"{k}_{var_suffix}" for k in ds.data_vars})
    except Exception as e:
        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ {typeOfLevel}: {e}")
        return None


# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–∏ —Å —Ñ–∏–ª—å—Ç—Ä–æ–º
def load_dataset_uv(typeOfLevel, var_suffix):
    try:
        ds = xr.open_dataset(
            grib_path,
            engine="cfgrib",
            decode_timedelta=False,
            backend_kwargs = {"indexpath": ""},
            filter_by_keys={'typeOfLevel': typeOfLevel, 'level': 10}
        )
        # –ü–µ—Ä–µ–∏–º–µ–Ω—É–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ —Å —Å—É—Ñ—Ñ–∏–∫—Å–æ–º, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –∫–æ–Ω—Ñ–ª–∏–∫—Ç–æ–≤
        return ds.rename({k: f"{k}_{var_suffix}" for k in ds.data_vars})
    except Exception as e:
        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ {typeOfLevel}: {e}")
        return None


def load_dataset_t(typeOfLevel, var_suffix):
    try:
        ds = xr.open_dataset(
            grib_path,
            engine="cfgrib",
            decode_timedelta=False,
            backend_kwargs = {"indexpath": ""},
            filter_by_keys={'typeOfLevel': typeOfLevel, 'level': 2}
        )
        # –ü–µ—Ä–µ–∏–º–µ–Ω—É–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ —Å —Å—É—Ñ—Ñ–∏–∫—Å–æ–º, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –∫–æ–Ω—Ñ–ª–∏–∫—Ç–æ–≤
        return ds.rename({k: f"{k}_{var_suffix}" for k in ds.data_vars})
    except Exception as e:
        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ {typeOfLevel}: {e}")
        return None



# –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–æ —É—Ä–æ–≤–Ω—è–º
ds_pressure = load_dataset("isobaricInhPa", "press")
ds_surface = load_dataset("surface", "surf")
ds_heightAboveGround = load_dataset_uv("heightAboveGround", "height")
ds_t = load_dataset_t("heightAboveGround", "t")
ds_mean_sea = load_dataset("meanSea", "msl")

# –û–±—ä–µ–¥–∏–Ω—è–µ–º
datasets = [ds for ds in [ds_pressure, ds_surface, ds_heightAboveGround, ds_t['t2m_t'], ds_mean_sea["prmsl_msl"]] if ds is not None]
ds_all = xr.merge(datasets, compat='override')

# –î–æ–±–∞–≤–∏–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏
ds_all = ds_all.expand_dims({"batch": [0], "time": [0]})

base_datetime_str = f"{date.strftime('%Y-%m-%d')}T{run_hour}:00"
ds_all = prepare_graphcast_coordinates(ds_all, base_datetime_str)

ds_all["orog_surf"] = ds_all["orog_surf"].squeeze(dim=["batch", "time"])
ds_all["lsm_surf"] = ds_all["lsm_surf"].squeeze(dim=["batch", "time"])

#ds_all["v10_height"] = ds_all["v10_height"].expand_dims({"batch": [0], "time": [0]})


print("üì¶ –ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –≤ ds_all:")
print(list(ds_all.data_vars))
print(list(ds_all.coords))

if "orog_surf" in ds_all:
    ds_all["orog_surf"] = ds_all["orog_surf"] * 9.80665

if "gh_press" in ds_all:
    ds_all["gh_press"] = ds_all["gh_press"] * 9.80665

# –ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –ø–æ–¥ GraphCast –∏–ª–∏ —Å–≤–æ–∏ –Ω—É–∂–¥—ã
rename_map = {
    "gh_press": "geopotential",
    "t_press": "temperature",
    "q_press": "specific_humidity",
    "w_press": "vertical_velocity",
    "u_press": "u_component_of_wind",
    "v_press": "v_component_of_wind",
    "t_surf": "surface_temperature",
    "prate_surf": "total_precipitation_6hr",
    "lsm_surf": "land_sea_mask",
    "orog_surf": "geopotential_at_surface",  # –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
    "isobaricInhPa": "level",
    "latitude": "lat",
    "longitude": "lon",
    "u10_height": "10m_u_component_of_wind",
    "v10_height": "10m_v_component_of_wind",
    "t2m_t": "2m_temperature",
    "prmsl_msl": "mean_sea_level_pressure"
}

# –ü—Ä–∏–º–µ–Ω—è–µ–º –ø–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ
ds_all = ds_all.rename(rename_map)

# –£–¥–∞–ª–∏–º –Ω–µ–Ω—É–∂–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
ds_all = ds_all.drop_vars(["batch", "heightAboveGround", "meanSea", "step", "surface", "surface_temperature", "valid_time"])


ds_all = ds_all.sortby("level", ascending=True)


# –ü—Ä–∏–≤–æ–¥–∏–º lat/lon –∫ float32
ds_all = ds_all.assign_coords({
    "lat": ds_all["lat"].astype(np.float32),
    "lon": ds_all["lon"].astype(np.float32)
})

# –£–¥–∞–ª—è–µ–º batch –∏–∑ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç (–¥–µ–ª–∞–µ–º –∫–∞–∫ –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª–µ)
if "batch" in ds_all.coords:
    ds_all = ds_all.swap_dims({"batch": "batch"})  # —Å–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å
    ds_all = ds_all.drop_vars("batch")  # —É–±–∏—Ä–∞–µ–º –∫–∞–∫ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—É

    # –£–±–∏—Ä–∞–µ–º time –∏ batch –∏–∑ geopotential_at_surface –∏ land_sea_mask
if set(ds_all["geopotential_at_surface"].dims) == {"batch", "time", "lat", "lon"}:
    ds_all["geopotential_at_surface"] = ds_all["geopotential_at_surface"].isel(batch=0, time=0)
if set(ds_all["land_sea_mask"].dims) == {"batch", "time", "lat", "lon"}:
    ds_all["land_sea_mask"] = ds_all["land_sea_mask"].isel(batch=0, time=0)

# –°–æ—Ö—Ä–∞–Ω—è–µ–º
out_path = f"c:/Users/obedenok/PycharmProjects/Graphcast/temp/test_{times.strftime('%Y-%m-%d_%H-%M')}.nc"
ds_all.to_netcdf(out_path)
print(f"‚úÖ –í—Å—ë —É—Å–ø–µ—à–Ω–æ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–æ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤ {out_path}")


#-----------------------------------------------------------------------------------------------
'''
# –ó–∞–≥—Ä—É–∂–∞–µ–º –æ–±–∞ —Ñ–∞–π–ª–∞
ds1 = xr.open_dataset("c:/Users/obedenok/PycharmProjects/graphcast_clean/dataset/gfs_1p00_f000_all1.nc")
ds2 = xr.open_dataset("c:/Users/obedenok/PycharmProjects/graphcast_clean/dataset/gfs_1p00_f000_all2.nc")

# –û–±—ä–µ–¥–∏–Ω—è–µ–º –ø–æ –æ—Å–∏ time
combined = xr.concat([ds1, ds2], dim="time")


# üéØ –í–æ—Å—Å—Ç–∞–Ω–æ–≤–∏–º datetime –∏ —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä—É–µ–º time
if "datetime" in ds1 and "datetime" in ds2:
    print("üõ† –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º datetime –∏ time...")

    # –ë–µ—Ä—ë–º datetime –∫–∞–∫ (batch, time)
    dt1 = ds1["datetime"]
    dt2 = ds2["datetime"]

    # –ü—Ä–æ–≤–µ—Ä–∏–º —Ä–∞–∑–º–µ—Ä—ã
    assert dt1.dims == ("batch", "time")
    assert dt2.dims == ("batch", "time")

    # –û–±—ä–µ–¥–∏–Ω—è–µ–º datetime –ø–æ –æ—Å–∏ –≤—Ä–µ–º–µ–Ω–∏ (axis=1)
    dt_combined = np.concatenate([dt1.values, dt2.values], axis=1)

    # –ù–∞–∑–Ω–∞—á–∞–µ–º datetime –∫–∞–∫ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—É
    combined["datetime"] = (("batch", "time"), dt_combined)
    combined = combined.set_coords("datetime")

    # –í–æ—Å—Å—Ç–∞–Ω–æ–≤–∏–º time –∫–∞–∫ timedelta –æ—Ç –ø–µ—Ä–≤–æ–≥–æ datetime
    base_datetime = dt_combined[0, 0]
    time_deltas = (dt_combined[0] - base_datetime) / np.timedelta64(1, "h")
    time_values = np.array(time_deltas, dtype="timedelta64[h]")

    # –ó–∞–º–µ–Ω–∏–º –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—É time
    combined = combined.assign_coords(time=("time", time_values))

    print("‚úÖ datetime –∏ time —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω—ã")
else:
    print("‚ö†Ô∏è –í –æ–¥–Ω–æ–º –∏–∑ —Ñ–∞–π–ª–æ–≤ –Ω–µ—Ç –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã 'datetime'")



# –£–¥–∞–ª—è–µ–º time –∏–∑ —Å—Ç–∞—Ç–∏—á–Ω—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö
for var in ["geopotential_at_surface", "land_sea_mask"]:
    if var in combined and "time" in combined[var].dims:
        print(f"‚öôÔ∏è –£–¥–∞–ª—è–µ–º –æ—Å—å 'time' –∏–∑ {var}")
        combined[var] = combined[var].isel(time=0, drop=True)

# –ò—Å–ø—Ä–∞–≤–ª—è–µ–º –ø–æ—Ä—è–¥–æ–∫ –æ—Å–µ–π
def fix_dims(da):
    dims = da.dims
    if "level" in dims:
        return da.transpose("batch", "time", "level", "lat", "lon")
    elif "time" in dims:
        return da.transpose("batch", "time", "lat", "lon")
    else:
        return da.transpose("lat", "lon")

# –ü—Ä–∏–º–µ–Ω—è–µ–º –∫–æ –≤—Å–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–º
for var in combined.data_vars:
    combined[var] = fix_dims(combined[var])

# –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã
for coord in list(combined.coords):
    if coord not in combined.dims and coord != "datetime":
        combined = combined.drop_vars(coord)

# –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∂–µ–ª–∞–µ–º—ã–π –ø–æ—Ä—è–¥–æ–∫ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö (–ø—Ä–∏–º–µ—Ä –∏–∑ —ç—Ç–∞–ª–æ–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞)
desired_order = [
    'geopotential_at_surface',
    'land_sea_mask',
    '2m_temperature',
    'mean_sea_level_pressure',
    '10m_v_component_of_wind',
    '10m_u_component_of_wind',
    'total_precipitation_6hr',
    'temperature',
    'geopotential',
    'u_component_of_wind',
    'v_component_of_wind',
    'vertical_velocity',
    'specific_humidity'
]

# –°–æ–±–∏—Ä–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç –∑–∞–Ω–æ–≤–æ –≤ –Ω—É–∂–Ω–æ–º –ø–æ—Ä—è–¥–∫–µ
combined_ordered = xr.Dataset({var: combined[var] for var in desired_order if var in combined.data_vars}, coords=combined.coords)

# –°–æ—Ö—Ä–∞–Ω—è–µ–º
combined_ordered.to_netcdf("c:/Users/obedenok/PycharmProjects/graphcast_clean/dataset/combined_gfs.nc")
print("‚úÖ –§–∞–π–ª —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ –∫–∞–∫ combined_gfs.nc")

#-------------------------------------------------------------------------------------------------

# –ó–∞–≥—Ä—É–∂–∞–µ–º –∏—Å—Ö–æ–¥–Ω—ã–π —Ñ–∞–π–ª
ds = xr.open_dataset("c:/Users/obedenok/PycharmProjects/graphcast_clean/dataset/combined_gfs.nc", decode_timedelta=True)

# –ü–æ–ª—É—á–∏–º —à–∞–≥ –≤—Ä–µ–º–µ–Ω–∏ (timedelta)
time_step = ds.time[1].item() - ds.time[0].item()
datetime_step = pd.to_datetime(ds.datetime.values[0, 1]) - pd.to_datetime(ds.datetime.values[0, 0])

# –°–∫–æ–ª—å–∫–æ –Ω–æ–≤—ã—Ö —à–∞–≥–æ–≤ –¥–æ–±–∞–≤–∏–º
n_new = 50

# –ù–æ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –≤—Ä–µ–º–µ–Ω–∏ –∏ –¥–∞—Ç—ã
new_time = ds.time.values[-1] + time_step * np.arange(1, n_new + 1)
new_datetime = pd.to_datetime(ds.datetime.values[0, -1]) + datetime_step * np.arange(1, n_new + 1)

# –°–æ–∑–¥–∞–¥–∏–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ —Å NaN —Ç–æ–ª—å–∫–æ –¥–ª—è –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö, —É –∫–æ—Ç–æ—Ä—ã—Ö –µ—Å—Ç—å –∏–∑–º–µ—Ä–µ–Ω–∏–µ "time"
new_data_vars = {}
for var in ds.data_vars:
    da = ds[var]
    if "time" in da.dims:
        dims = da.dims
        shape = list(da.shape)
        shape[dims.index("time")] = n_new  # –∑–∞–º–µ–Ω–∏–º —Ä–∞–∑–º–µ—Ä –æ—Å–∏ –≤—Ä–µ–º–µ–Ω–∏

        # –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã, –∫—Ä–æ–º–µ "datetime", –∏–Ω–∞—á–µ –±—É–¥–µ—Ç –æ—à–∏–±–∫–∞ –ø—Ä–∏ concat
        coords = {k: v for k, v in da.coords.items() if k in dims and k != "datetime"}
        coords["time"] = new_time

        new_da = xr.DataArray(
            data=np.full(shape, np.nan, dtype=da.dtype),
            dims=dims,
            coords=coords
        )

        # –£–±–∏—Ä–∞–µ–º –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ–≥–æ —Å–ª–∏—è–Ω–∏—è
        if "datetime" in da.coords:
            da = da.reset_coords(names="datetime", drop=True)

        # –û–±—ä–µ–¥–∏–Ω—è–µ–º
        new_data_vars[var] = xr.concat([da, new_da], dim="time")
    else:
        new_data_vars[var] = da  # –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π

# –û–±—ä–µ–¥–∏–Ω—è–µ–º time –∏ datetime –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã
new_time_full = np.concatenate([ds.time.values, new_time])
new_datetime_full = np.concatenate([ds.datetime.values[0], new_datetime])
datetime_expanded = np.expand_dims(new_datetime_full, axis=0)

# –°–æ–∑–¥–∞—ë–º –∏—Ç–æ–≥–æ–≤—ã–π Dataset
ds_out = xr.Dataset(new_data_vars)
ds_out = ds_out.assign_coords(time=("time", new_time_full))
ds_out = ds_out.assign_coords(datetime=(("batch", "time"), datetime_expanded))

# –µ—Å–ª–∏ batch –±—ã–ª–∞ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–æ–π
if "batch" in ds.coords:
    ds_out = ds_out.assign_coords(batch=ds.coords["batch"])

# –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
ds_out.to_netcdf(f"w:/Postprocesing/Oleh Bedenok/GRAPHCAST/NOAA/dataset_NOAA1/NOAA_{datt}.nc", format="NETCDF3_CLASSIC")
print(f"‚úÖ –ì–æ—Ç–æ–≤–æ: –¥–∞–Ω–Ω—ã–µ —Ä–∞—Å—à–∏—Ä–µ–Ω—ã –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ NOAA_{datt}.nc")'''
